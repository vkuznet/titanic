# Titanic: Machine Learning from Disaster

DATASCI 450: Deriving Knowledge from Data at Scale
Authors: V. Kuznetsov, J. Arturo Covarrubias

## Problem statement

Predict survival on the Titanic

### Data analysis

We used all available tools for this data analysis, including R, Weka, Python.
The data clean-up and preparation was done in R, the modeling was done both in R and Weka.
The bash shell scripts and python were used for batching purposes (see later).

The team **valya** was registered by one of us (V. Kuznetsov) under the kaggle web site:

<div id="container">
    <img class=center src=images/kaggle_team.png />
    Kaggle team profile
</div>

### Part 1

In section we cover model description, answering the following questions:

- what feature looks most promising for modeling
- do you need to deal with missing values?
- scale the attributes
- compare with multiple classifiers
- kaggle model

We started from data exploration by plotting various attributes from provided
data set. The most interested plots are shown below:

<div id="container">
    <img class=center src=images/mosaic.png />
    Fig 1. Mosaic plot of survival rate for Class, Sex and Age attributes.
</div>

The data survival rate plot, see Fig 1, clearly indicated that
survival rate increases significantly for females and children.
Most children were in 2nd and 3rd class. Most females in first and second
classes, while only half of them survived from the third class.

Then we looked at different class distributions. Here is class vs SibSp and Parch
attributes:
<div id="container">
    <img class=center src=images/class_plots1.png />
    Fig 2. Survival rate for Class, SibSp and Parch attributes.
</div>

<div id="container">
    <img class=center src=images/class_plots2.png />
    Fig 3. Survival rate for Class, Age and Embarked attributes.
</div>

<div id="container">
    <img class=center src=images/class_plots3.png />
    Fig 4. Survival rate for Class and Sec attributes
</div>

Based on aforementioned plots we concluded that Age, Sex, Class are most powerful
attributes. Below you can see a plot how they behave upon applying the cut:
*age<18&class<3|sex=F&class<3|non-embarked*:

<div id="container">
    <img class=center src=images/class_plots4.png />
    Fig 5. Survival rates for age/class/sec/embarked attributes with
    and without imposed cut.
</div>

Here is correlation matrix for for current set of attributes:

<div id="container">
    <img class=center src=images/cor.png />
    Fig 6. Correlation matrix
</div>

We run multiple Weka ML algorithms, including NaiveBayes, J48, RandomForest, SMO,
AdaBoostM1, etc. Here their initial benchmarks:

```
===> run weka.NaiveBayes.sh
Correctly Classified Instances         688               77.2166 %
Kappa statistic                          0.4917
Correctly Classified Instances         687               77.1044 %
Kappa statistic                          0.4889

===> run weka.j48.sh
Correctly Classified Instances         779               87.4299 %
Kappa statistic                          0.7245
Correctly Classified Instances         716               80.3591 %
Kappa statistic                          0.5761

===> run weka.RandomForest.sh
Correctly Classified Instances         871               97.7553 %
Kappa statistic                          0.9522
Correctly Classified Instances         717               80.4714 %
Kappa statistic                          0.5764

===> run weka.BFTree.sh
Correctly Classified Instances         722               81.0325 %
Kappa statistic                          0.5865
Correctly Classified Instances         709               79.5735 %
Kappa statistic                          0.5569

===> run weka.SMO.sh
Correctly Classified Instances         701               78.6756 %
Kappa statistic                          0.5421
Correctly Classified Instances         701               78.6756 %
Kappa statistic                          0.5421

===> run weka.Vote.sh
Correctly Classified Instances         723               81.1448 %
Kappa statistic                          0.5901
Correctly Classified Instances         708               79.4613 %
Kappa statistic                          0.5572

===> run weka.AdaBoostM1.sh
Correctly Classified Instances         891              100      %
Kappa statistic                          1
Correctly Classified Instances         712               79.9102 %
Kappa statistic                          0.5635
```

The initial submission was made using Weka RandomForest model. We received
0.73206 score with position 5780:
<div id="container">
    <img class=center src=images/kaggle1.png />
    Kaggle submission 1.
</div>

It gave the following classification:

```
Correctly Classified Instances         890               99.8878 %
Incorrectly Classified Instances         1                0.1122 %
Root mean squared error                  0.142

while with cross-validation it had

Correctly Classified Instances         725               81.3692 %
Incorrectly Classified Instances       166               18.6308 %
Root mean squared error                  0.3768
```

### Part 2

- model improvement
  - missing values
  - scale attributes
  - bins
  - new features

- describe data cleaning, transformation
- kaggle models

### Part 3

- multiple classifiers, different model settings
- screen-shots
- model strategy
- model ensembles

## Kaggle models

The second submission was due to assignment of Age based on review of
SibSp and Parch attributes as well as adding CabinId, TicketId variables. The later was
just numerical values of Cabin and Ticket factors in R. For this submission we used
R RandomForest implementation which scored almost 95% accuracy on the training set.
At this step we jumped to position 2829 with score 0.77512:
<div id="container">
    <img class=center src=images/kaggle2.png />
Fig 1. Kaggle submission 2.
</div>

Kaggle submission 3 and 4 were based on improvements with cabin assignment and
matching ticket ids with cabin categories. At this stage we run in parallel R
KSVM (with RBFKernel) and R RandomForest ML algorithms. Due to these changes we
jumped up 2154 points in a leadership board with score 0.78947 and position 678:
<div id="container">
    <img class=center src=images/kaggle3a.png />
Fig 1. Kaggle score 3.
</div>

Kaggle submission 5, 6 and 7 were mostly fine-tuning of SVM and RandomForest models.
They did not improve our final score.

Kaggle submission 8 was based on introduction of new Married attributed, which
was assigned as following: 1 for names with Miss. in them, 2 for Mrs., 3 for
Mr.  and zero otherwise. The RF model shown an percentage improvement and we
submit our results. On a leadership board we jumped up 1.435% to score 0.80383
and position 145:

<div id="container">
    <img class=center src=images/kaggle4.png />
Fig 1. Kaggle score 4.
</div>

## References

1. The code for this assignment can be found at
   https://github.com/vkuznet/titanic
