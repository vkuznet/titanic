# Titanic: Machine Learning from Disaster

DATASCI 450: Deriving Knowledge from Data at Scale
Authors: V. Kuznetsov, J. Arturo Covarrubias

## Problem statement

Predict survival on the Titanic

### Data analysis

We used all available tools for this data analysis, including R, Weka, Python.
The data clean-up and preparation was done in R, the modeling was done both in R and Weka.
The bash shell scripts and python were used for batching purposes (see later).

The team **valya** was registered by one of us (V. Kuznetsov) under the kaggle web site:

<div id="container">
    <img class=center src=images/kaggle_team.png />
    Kaggle team profile
</div>

### Part 1

In section we cover model description, answering the following questions:

- what feature looks most promising for modeling
- do you need to deal with missing values?
- scale the attributes
- compare with multiple classifiers
- kaggle model

We started from data exploration by plotting various attributes from provided
data set. The most interested plots are shown below:

<div id="container">
    <img class=center src=images/mosaic.png />
    Fig 1. Mosaic plot of survival rate for Class, Sex and Age attributes.
</div>

The data survival rate plot, see Fig 1, clearly indicated that
survival rate increases significantly for females and children.
Most children were in 2nd and 3rd class. Most females in first and second
classes, while only half of them survived from the third class.

Then we looked at different class distributions. Here is class vs SibSp and Parch
attributes:
<div id="container">
    <img class=center src=images/class_plots1.png />
    Fig 2. Survival rate for Class, SibSp and Parch attributes.
</div>

<div id="container">
    <img class=center src=images/class_plots2.png />
    Fig 3. Survival rate for Class, Age and Embarked attributes.
</div>

<div id="container">
    <img class=center src=images/class_plots3.png />
    Fig 4. Survival rate for Class and Sec attributes
</div>

Based on aforementioned plots we concluded that Age, Sex, Class are the most powerful
attributes. Below you can see a plot how they behave upon applying the cut:
*age<18&class<3|sex=F&class<3|non-embarked*:

<div id="container">
    <img class=center src=images/class_plots4.png />
    Fig 5. Survival rates for age/class/sec/embarked attributes with
    and without imposed cut.
</div>

Here is correlation matrix for for current set of attributes:

<div id="container">
    <img class=center src=images/cor.png />
    Fig 6. Correlation matrix
</div>

We run multiple Weka ML algorithms, including NaiveBayes, J48, RandomForest, SMO,
AdaBoostM1, etc. Here their initial benchmarks:

```
===> run weka.NaiveBayes.sh
Correctly Classified Instances         688               77.2166 %
Kappa statistic                          0.4917
Correctly Classified Instances         687               77.1044 %
Kappa statistic                          0.4889

===> run weka.j48.sh
Correctly Classified Instances         779               87.4299 %
Kappa statistic                          0.7245
Correctly Classified Instances         716               80.3591 %
Kappa statistic                          0.5761

===> run weka.RandomForest.sh
Correctly Classified Instances         871               97.7553 %
Kappa statistic                          0.9522
Correctly Classified Instances         717               80.4714 %
Kappa statistic                          0.5764

===> run weka.BFTree.sh
Correctly Classified Instances         722               81.0325 %
Kappa statistic                          0.5865
Correctly Classified Instances         709               79.5735 %
Kappa statistic                          0.5569

===> run weka.SMO.sh
Correctly Classified Instances         701               78.6756 %
Kappa statistic                          0.5421
Correctly Classified Instances         701               78.6756 %
Kappa statistic                          0.5421

===> run weka.Vote.sh
Correctly Classified Instances         723               81.1448 %
Kappa statistic                          0.5901
Correctly Classified Instances         708               79.4613 %
Kappa statistic                          0.5572

===> run weka.AdaBoostM1.sh
Correctly Classified Instances         891              100      %
Kappa statistic                          1
Correctly Classified Instances         712               79.9102 %
Kappa statistic                          0.5635
```

The initial submission was made using Weka RandomForest model. We received
0.73206 score with position 5780:
<div id="container">
    <img class=center src=images/kaggle1.png />
    Kaggle submission 1.
</div>

It gave the following classification:

```
Correctly Classified Instances         890               99.8878 %
Incorrectly Classified Instances         1                0.1122 %
Root mean squared error                  0.142

while with cross-validation it had

Correctly Classified Instances         725               81.3692 %
Incorrectly Classified Instances       166               18.6308 %
Root mean squared error                  0.3768
```

### Part 2

- model improvement
  - missing values
  - scale attributes
  - bins
  - new features

- describe data cleaning, transformation
- kaggle models

We investigated several options for model improvement:
- scale Age and Fare attributes
- merged Parch and SibSp attrubutes
- performed data cleaning, i.e. assign missing Age values, by loading data
  from external source (Titanica web site [2]) and perform matching based on
  ticket and name attributes
- introduced a new set of attributes such as, binary Child attribute, Ticket id,
  cabin categories (A, B, C, D, etc.) as well as binary attribute for having a
  cabin
- we also performed K-means clustering for our data

Based on Age assignment as well as cabin-id and ticket-id matching
and using RandomForest ML algorithm
we're able to reach position 2829 with score 0.77512
<div id="container">
    <img class=center src=images/kaggle2.png />
Fig 1. Kaggle submission 2.
</div>

Further, we investigated various SVM models and use the one based on RBFKernel.
We performed cabin-id/ticket-id assingments and were able to jump up
2154 points in a leadership board with score 0.78947 and position 678:
<div id="container">
    <img class=center src=images/kaggle3a.png />
Fig 1. Kaggle score 3.
</div>

Later we introduced a new Title attribute: Miss, Master, Mr, Mrs. This help us to improve
our results by 1.435% and we jumped to score 0.80383 and position 145:

<div id="container">
    <img class=center src=images/kaggle4.png />
Fig 1. Kaggle score 4.
</div>

By adding clustering attribute (based on 7 clusters) we further improved our results by
half of the percent up to 0.80861 and position 107:

<div id="container">
    <img class=center src=images/kaggle5.png />
Fig 1. Kaggle score 5.
</div>

### Part 3

We tried different classifiers, including BFTree, NaiveBayes, RandomForest, SMO,
Stacking, AdaBoostM1, Voting and J48. Those were used in Weka, while we
test as well SVM and RandomForest R implementations. We observed the overfiting
problem with AdaBoostM1 and RandomForest which were able to classify data
almost 100% on a training set, but had 80% success with 10-fold cross-validation.
We confirmed that empirically by submitting our prediction to kaggle and observed that
model based on full training set performed poorly with respect to ones based on
10-fold cross validation. For instance, here is results from AdaBoostM1 classification:

```
===> run weka.AdaBoostM1.sh
Correctly Classified Instances         891              100      %
Kappa statistic                          1
Correctly Classified Instances         712               79.9102 %
Kappa statistic                          0.5635
```

The first numbers show that we're able to successfully prediction all cases using training
set, while second number (79.9%) show very poor performance during cross-validation.

Further trials with different attribute settings did not improve our results. Here is our
three best submissions:

<div id="container">
    <img class=center src=images/kaggle_trial1.png />
    <img class=center src=images/kaggle_trial2.png />
    <img class=center src=images/kaggle_trial3.png />
</div>

The current position at leadership board is the following:
<div id="container">
    <img class=center src=images/kaggle_current.png />
</div>


## Kaggle models


## References

1. The code for this assignment can be found at
   https://github.com/vkuznet/titanic
2. Titanica web site: http://www.encyclopedia-titanica.org/
