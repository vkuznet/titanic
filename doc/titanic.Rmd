# Titanic: Machine Learning from Disaster

**DATASCI 450: Deriving Knowledge from Data at Scale**

*Authors: V. Kuznetsov, J. Arturo Covarrubias*

## Problem statement

Predict survival on the Titanic

The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew.  This sensational tragedy shocked the international community and led to better safety regulations for ships.

One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew.  Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.

In this contest, we ask you to complete the analysis of what sorts of people were likely to survive.  In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy [3].

**Started: 9:13 pm, Friday 28 September 2012 UTC**

**Ends: 12:00 am, Saturday 28 September 2013 UTC**


### Data analysis

We used all available tools for this data analysis, including R, Weka, Python.
The data clean-up and preparation was done in R, the modeling was done both in R and Weka.
The bash shell scripts and python were used for batching purposes. We also used
github for code repository and data management [1].

The team **valya** was registered by one of us (V. Kuznetsov) on the kaggle web site:

<div id="container">
    <img class=center src=images/kaggle_team.png />
    Kaggle team profile
</div>

### Part 1

This section covers model description, including:

- what feature looks most promising for modeling
- do you need to deal with missing values?
- scale the attributes
- compare with multiple classifiers
- kaggle model

We started from the data exploration by plotting various attributes from provided
data set. The most interested plots are shown below:

<div id="container">
    <img class=center src=images/mosaic.png />
    Fig 1. Mosaic plot of survival rate for Class, Sex and Age attributes.
</div>

The mosaic plot, see Fig 1, clearly indicated that survival rate significantly
increases for females and children.  Most survived children were in 2nd and 3rd
class, while females from the first and second classes had better change to
survive with only half of them survived from the third class.

It was also useful to look at mosaic plot of cabin, fare, embarked attributes:
<div id="container">
    <img class=center src=images/mosaic2.png />
    Fig 1. Mosaic plot of survival rate for Cabin, Fare and Embarked attributes.
</div>

Here we binned Fare attribute as Low (<10), Medium (10-50) and High(>50) as well
as assigned binary attribute for having a Cabin.

Then we looked at different class distributions. Here is class vs SibSp and Parch
attributes:
<div id="container">
    <img class=center src=images/class_plots1.png />
    Fig 2. Survival rate for Class, SibSp and Parch attributes.
</div>

<div id="container">
    <img class=center src=images/class_plots2.png />
    Fig 3. Survival rate for Class, Age and Embarked attributes.
</div>

<div id="container">
    <img class=center src=images/class_plots3.png />
    Fig 4. Survival rate for Class and Sex attributes
</div>

Based on aforementioned plots we concluded that Age, Sex, Class attributes are
the most powerful attributes. Below you can see a plot how survival rate changes
when we apply the following cut: *age<18&class<3|sex=F&class<3|non-embarked*:

<div id="container">
    <img class=center src=images/class_plots4.png />
    Fig 5. Survival rates for age/class/sec/embarked attributes with
    and without imposed cut.
</div>

Here is correlation matrix for for current set of attributes:

<div id="container">
    <img class=center src=images/cor.png />
    Fig 6. Correlation matrix
</div>

We run multiple Weka ML algorithms, including NaiveBayes, J48, RandomForest, SMO,
AdaBoostM1, etc. Here their initial benchmarks:

```
---------------------------------------------------------------------------------------------
| NaiveBayes | j48        | RandomForest | BFTree     | SMO       | Vote       | AdaBoostM1 |
---------------------------------------------------------------------------------------------
| 77% (0.49) | 87% (0.72) | 97% (0.95)   | 81% (0.58) | 78 (0.54) | 81% (0.59) | 100% (1)   |
---------------------------------------------------------------------------------------------
| 77% (0.49) | 80% (0.57) | 80% (0.57)   | 79% (0.55) | 78 (0.54) | 79% (0.55) | 79% (0.56) |
---------------------------------------------------------------------------------------------
```

The first row shows results based on a training set, while second corresponds to results
from 10-fold cross-validation output.

The initial submission was made using Weka RandomForest model. We received
0.73206 score with position 5780:
<div id="container">
    <img class=center src=images/kaggle1.png />
    Kaggle submission 1.
</div>

It had the following classification: 99.8%/81.4% for training and cross-validation
outputs.

### Part 2

This section describes model improvements and various features selections, including:

- model improvement
  - missing values
  - scale attributes
  - bins
  - new features

- describe data cleaning, transformation
- kaggle models

We investigated several options to improve our model:
- scale Age and Fare attributes
- merged Parch and SibSp attributes
- data cleaning with assigning missing Age values by loading data
  from external source (Titanica web site [2]) and perform matching based on
  ticket and name attributes
- exploring new set of attributes such as, binary Child attribute, Ticket id,
  cabin categories (A, B, C, D, etc.) as well as binary attribute for having a
  cabin
- we also performed K-means clustering for our data

Based on Age assignment as well as cabin-id and ticket-id matching and using
RandomForest ML algorithm we're able to reach position 2829 with score 0.77512
<div id="container">
    <img class=center src=images/kaggle2.png />
Kaggle submission 2.
</div>

Further, we investigated various SVM models and use the one based on RBFKernel.
We performed cabin-id/ticket-id assignment and were able to jump up
2154 points in a leadership board with a score 0.78947 and position 678:
<div id="container">
    <img class=center src=images/kaggle3a.png />
Kaggle score 3.
</div>

Later, we introduced a new Title attribute: Miss, Master, Mr, Mrs. This help us to improve
our results by 1.435% and our score jumped to 0.80383 and position 145:

<div id="container">
    <img class=center src=images/kaggle4.png />
Kaggle score 4.
</div>

By adding clustering attribute (based on 7 clusters) we further improved our results by
half of the percent up to 0.80861 and position 107:

<div id="container">
    <img class=center src=images/kaggle5.png />
Kaggle score 5.
</div>

### Part 3

This section describes our attempts to use different classifiers.
We used: BFTree, NaiveBayes, RandomForest, SMO,
Stacking, AdaBoostM1, Voting and J48 in Weka as well as
SVM and RandomForest from R implementations. We observed the over-fitting
problem with AdaBoostM1 and RandomForest which were able to classify data
almost 100% on a training set, but only had ~80% success with 10-fold cross-validation.
We confirmed that empirically by submitting our prediction to kaggle and observed that
model based on full training set performed poorly with respect to ones based on
10-fold cross validation. For instance, AdaBoostM1 classification had the following:
100% on a training set with kappa statistics equal to 1, while only had 79% with kappa
statistics 0.56 using 10-fold cross-validation sample.

Further trials with different attribute settings did not improve our results. Here is our
three best submissions:

<div id="container">
    <img class=center src=images/kaggle_trial1.png />
    <img class=center src=images/kaggle_trial2.png />
    <img class=center src=images/kaggle_trial3.png />
</div>

The current position on a leadership board is the following:
<div id="container">
    <img class=center src=images/kaggle_current.png />
</div>

Our code is available on github [1].

## References

1. The code for this assignment can be found at
   https://github.com/vkuznet/titanic
2. Titanica web site: http://www.encyclopedia-titanica.org/
3. Kaggle Site : https://www.kaggle.com/c/titanic-gettingStarted
